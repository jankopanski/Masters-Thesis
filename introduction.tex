\documentclass[thesis-en.tex]{subfiles}

\begin{document}
\paragraph{Background}
% entered the age of
With the deployment of Fugaku \cite{dongarra2020report}, supercomputing has already exceeded the threshold of exascale computing. Nevertheless, High-performance computing (HPC) is still facing the challenge of a constantly growing performance gap between compute resources and I/O capabilities. HPC applications typically alternate between compute-intensive and I/O-intensive execution phases, where the latter is characterised by emitting bursty I/O requests. Those I/O spikes produced by multiple parallel jobs can saturate network bandwidth and lead to I/O congestion, which effectively stretches the I/O phases of applications resulting in higher turnaround time. However, the development of novel storage technologies, such as NVRAM, pave the way to solve the issue of bursty I/O by the introduction of burst buffers \cite{6232369}.

Burst buffer is an intermediate fast persistent storage layer, which is logically positioned between random-access main memory in compute nodes and a parallel file system (PFS). Burst buffers enable to immediately absorb bursty I/O requests and gradually flush them to the PFS. This application provides the possibility to implement checkpointing in HPC platforms efficiently. There are, however, several other applications of burst buffers, among others, data staging, write-through cache or in-situ analysis.

Since the introduction of the burst buffer concept, many supercomputers have been equipped with high-performance storage devices, mostly in the form of Non-Volatile Memory Express (NVMe) Solid State Drives (SSD). They have also been configured in various architectures, where the most prominent are node-local burst buffers as in Summit \cite{osti_1489443} and remote shared burst buffers as in Cori \cite{bhimji2016accelerating}. Simultaneously, numerous research publications on I/O-aware scheduling has been released, which span from methods of avoiding I/O contentions \cite{7307592,10.1145/2907294.2907316}, through deciding whether or not to use burst buffers \cite{8752797} and ending on improving the efficiency of high-level job scheduling \cite{10.1145/3307681.3325401}. 

Despite the intensive development of real supercomputing platforms and appearance of novel research algorithms, Resources and Jobs Management Systems (RJMS) provide only basic software support for burst buffer utilisation. RJMSs, such as Slurm, usually implement some sort of backfilling algorithm for online job scheduling (online batch scheduling). We observed that even though HPC platforms have been equipped with the new kind of resources - burst buffers, the process of allocating compute resources for future reservations in backfilling has not been extended with storage resources. This issue may potentially be a cause of starvation and deteriorate the performance of scheduling. Therefore, we dedicate this dissertation to study burst-buffer--aware scheduling algorithms and evaluate their efficiency in a simulated environment.

\paragraph{Simulation}
In order to test and compare scheduling algorithm, we have created a detailed supercomputer simulator, based on Batsim \cite{dutot:hal-01333471} and SimGrid \cite{casanova:hal-01017319}, which is capable of simulating I/O contention and I/O congestion effects. In fact, we developed two simulation models: one focused on simulating allocation of resources, and the other that extends it with I/O side effects. We modelled the SimGrid platform network to resemble a relatively small HPC cluster in a Dragonfly topology \cite{kim2008technology}. The chosen size of the cluster enabled us to perform an in-depth analysis of scheduling traces. This work is confined to research the job scheduling in the shared burst buffer architecture, which we modelled to resemble the architecture utilised in Fugaku.

One of the challenges was the availability of workload logs. The Parallel Workload Archive \cite{FEITELSON20142967} is known from a grand collection of computer workload logs gathered over that last two decades, which is an indispensable data source for scheduling research. Nevertheless, it does not contain information about burst buffer requests associated with parallel jobs, neither does any well-know publicly available dataset. Hence, we created a model of burst buffer request size distribution using the information about the requested main memory from the workload logs.

\paragraph{Online job scheduling}
Our first major contribution is the study of the impact of future burst buffer reservations on the overall efficiency of job scheduling. For this part, we experiment with four variants of First-Come-First-Served (FCFS) scheduling characterised with different approaches to resource reservations in aggressive backfilling (EASY backfilling). Namely, we compare FCFS (1) without backfilling, (2) with backfilling and future reservations for only compute resources (canonical backfilling), (3) with backfilling and reservations for both compute and storage resources, and lastly a (4) greedy filling, which may be perceived as a pure backfilling without any reservations. As we tackle the online scheduling, we study all the results based on four well-known user-centric metrics: waiting time, turnaround time (response time), slowdown (stretch) and bounded slowdown. 

The next major contribution is the proposal of new burst-buffer--aware scheduling algorithms, out of which the best performing is plan-based scheduling with simulated annealing optimisation. Based on real workload logs, we prove that it achieves a significant reduction of mean waiting time and mean bounded slowdown while maintaining a comparable statistical dispersion of waiting times. We collocate its results with FCFS backfilling and Shortest-Jobs-First (SJF) backfilling.

Additionally, we study the influence of backfilling reservation depth - the number of jobs at the front of a waiting queue that is prioritised in the backfilling and assigned with future reservations of resources.

\paragraph{Thesis organisation}
The reminder of this dissertation is organised as follows. In \autoref{ch:related-work}, we present an in-depth description of related work. We discuss, use cases of burst buffers, architectures of burst buffers in supercomputers, hardware implementation and provide a detailed overview of job scheduling terminology. In \autoref{ch:simulation-model}, we present the simulation model, its assumptions and the created workload. \autoref{ch:algorithms}, defines the canonical scheduling algorithms and also introduce three novel burst-buffer--aware scheduling algorithms. All described algorithms are followed with their evaluation in \autoref{ch:results}, where we present our analysis of experiments. In \autoref{ch:conclusion}, we form all conclusions arising from this dissertation. In \autoref{ch:future-work}, we propose several future research directions. Lastly, in \autoref{ch:online}, we gathered all online resources which were immensely helpful for us. We decided to share them as we find that they may be useful for related research.

\bigskip

\noindent
\textbf{All code sources associated with this dissertation are available at:}\\ \url{https://github.com/jankopanski/Burst-Buffer-Scheduling}
\end{document}